{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a0d570-ee2e-4d10-8fa7-d2c74e0d0a32",
   "metadata": {},
   "source": [
    "# Exercise 4 Custom: Assembling your own graph from Wikidata\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We are now going to assemble our own graph based on our own search terms!  This exercise is really just for fun.  Make it your own!  Be sure in particular to adjust your P-codes to reflect claims that are appropriate to your graph.\n",
    "\n",
    "The remainder of the course will use our pre-populated graphs.  This code is really just the framework that we used before.  And as a reminder, don't get frustrated if Wikidata doesn't cooperate.  Get a cup of coffee, restart the kernel, and try again.  Have fun!  :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f592899-24ea-49a3-b029-6fbf51f747d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import re\n",
    "import urllib\n",
    "from pprint import pprint\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "from pywikibot.data import api\n",
    "import pywikibot\n",
    "import wikipedia\n",
    "\n",
    "print(spacy.__version__)\n",
    "print(pywikibot.__version__)\n",
    "print(wikipedia.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe25f6-f1c8-4d2a-8e25-5788a72ca63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_nc = spacy.load('en_core_web_md')\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe('merge_noun_chunks')\n",
    "\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a45bd8-f9c3-4c20-a339-3ff589f0380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your own search term here\n",
    "# Be sure to check what spacy identifies as the named entities.  Make sure that there are\n",
    "# enough of them to make for an interesting graph.  But remember that the more you have \n",
    "# the longer it will take to query Wikidata.\n",
    "\n",
    "text = wikipedia.summary('')\n",
    "doc = nlp(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e07a8c-aa9f-4c4f-acc8-4a518f4232a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_ignore_ls = ['DATE']\n",
    "ner_list = []\n",
    "\n",
    "for el in doc.ents:\n",
    "    if el.label_ not in ent_ignore_ls:\n",
    "        #print(el, el.label_)\n",
    "        if el.text not in ner_list:\n",
    "            temp_doc = nlp(el.text)\n",
    "            ner_list.append(el.text)\n",
    "\n",
    "ner_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b473163-8d61-48e8-979e-d6dd6b9af4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    \n",
    "    regex = re.compile(r'[\\n\\r\\t]')\n",
    "    clean_text = regex.sub(\" \", text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def remove_stop_words_and_punct(text, print_text=False):\n",
    "    \n",
    "    result_ls = []\n",
    "    rsw_doc = non_nc(text)\n",
    "    \n",
    "    for token in rsw_doc:\n",
    "        if print_text:\n",
    "            print(token, token.is_stop)\n",
    "            print('--------------')\n",
    "        if not token.is_stop and not token.is_punct and not token.is_space:\n",
    "            result_ls.append(str(token))\n",
    "    \n",
    "    result_str = ' '.join(result_ls)\n",
    "\n",
    "    return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628b2a58-8e1e-4649-89d1-51f7b0f37538",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_text_ls = []\n",
    "\n",
    "for el in ner_list:\n",
    "    clean_text = remove_special_characters(el)\n",
    "    no_sw = remove_stop_words_and_punct(clean_text)\n",
    "    if no_sw not in node_text_ls:\n",
    "        node_text_ls.append(no_sw)\n",
    "\n",
    "node_text_ls = [node for node in node_text_ls if node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d5eb97-4c89-4160-8872-fd5c1f56471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getItems(site, itemtitle):\n",
    "    params = { 'action' :'wbsearchentities' , 'format' : 'json' , 'language' : 'en', 'type' : 'item', 'search': itemtitle}\n",
    "    request = api.Request(site=site,**params)\n",
    "    return request.submit()\n",
    "\n",
    "def getItem(site, wdItem, token):\n",
    "    request = api.Request(site=site,\n",
    "                          action='wbgetentities',\n",
    "                          format='json',\n",
    "                          ids=wdItem)    \n",
    "    return request.submit()\n",
    "\n",
    "def prettyPrint(variable):\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(variable)\n",
    "\n",
    "# Login to wikidata\n",
    "token = open('.wiki_api_token').read()\n",
    "wikidata = pywikibot.Site('wikidata', 'wikidata')\n",
    "site = pywikibot.Site(\"wikidata\", \"wikidata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c9bf2a-854e-4c0d-834c-baef5032d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "item_ls = []\n",
    "i = 0\n",
    "\n",
    "for el in node_text_ls:\n",
    "\n",
    "    wikidataEntries = getItems(site, el)\n",
    "    try:\n",
    "        tup = (wikidataEntries['search'][0]['id'], el)\n",
    "        item_ls.append(tup)\n",
    "    except:\n",
    "        i += 1\n",
    "        print('Missing ', i,'th entry for ', el)\n",
    "    \n",
    "dedup_item_ls = []\n",
    "\n",
    "for item in item_ls:\n",
    "    if item not in dedup_item_ls:\n",
    "        dedup_item_ls.append(item)\n",
    "        \n",
    "dedup_item_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a7b357-ee33-40f2-928a-0ca77e3667e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Choose some P-codes that are appropriate to your initial search\n",
    "# Be sure the values of the dict have no spaces (ex: 'field_of_work')\n",
    "\n",
    "p_dc = {\n",
    "    \n",
    "       }\n",
    "\n",
    "full_node_tup_ls = []\n",
    "\n",
    "for el in tqdm(item_ls):\n",
    "    itempage = pywikibot.ItemPage(wikidata, el[0])\n",
    "    itemdata = itempage.get()\n",
    "    source_node = itemdata['labels']['en']\n",
    "    #print(el, source_node)\n",
    "\n",
    "    for key in p_dc.keys():\n",
    "        #print(source_node, key, p_dc[key])\n",
    "        #print(itemdata['claims'])\n",
    "        try:\n",
    "            for i in itemdata['claims'][key]:\n",
    "                target = i.getTarget()\n",
    "                #print(target.id)\n",
    "                tup = (source_node, el[0], key, p_dc[key], target.labels['en'], target.id)\n",
    "                if tup not in full_node_tup_ls:\n",
    "                    full_node_tup_ls.append(tup)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb47c8e-432d-41c9-8121-ed1030e24e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neo4jConnection:\n",
    "    \n",
    "    def __init__(self, uri, user, pwd):\n",
    "        self.__uri = uri\n",
    "        self.__user = user\n",
    "        self.__pwd = pwd\n",
    "        self.__driver = None\n",
    "        try:\n",
    "            self.__driver = GraphDatabase.driver(self.__uri, auth=(self.__user, self.__pwd))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create the driver:\", e)\n",
    "        \n",
    "    def close(self):\n",
    "        if self.__driver is not None:\n",
    "            self.__driver.close()\n",
    "        \n",
    "    def query(self, query, parameters=None, db=None):\n",
    "        assert self.__driver is not None, \"Driver not initialized!\"\n",
    "        session = None\n",
    "        response = None\n",
    "        try: \n",
    "            session = self.__driver.session(database=db) if db is not None else self.__driver.session() \n",
    "            response = list(session.run(query, parameters))\n",
    "        except Exception as e:\n",
    "            print(\"Query failed:\", e)\n",
    "        finally: \n",
    "            if session is not None:\n",
    "                session.close()\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a482e6f6-2bf3-4be4-b0ed-beba064556d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = ''\n",
    "user = 'neo4j'\n",
    "pwd = ''\n",
    "\n",
    "conn = Neo4jConnection(uri=uri, user=user, pwd=pwd)\n",
    "conn.query(\"MATCH (n) RETURN COUNT(n)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d71be-2daf-4db3-bff3-1639c20e57ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.query('CREATE CONSTRAINT q_value IF NOT EXISTS ON (n:Node) ASSERT n.id IS UNIQUE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca063503-6756-4b5b-8843-b94696d4a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_node_tup_ls[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5743fb58-b6a9-4912-9909-a9b59c120c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(full_node_tup_ls, columns=['source_name', 'source_q', 'rel_p', 'rel_name', 'target_name', 'target_q'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718250ef-7cb3-48db-81e8-f5e240a06df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = df[['source_name', 'source_q']].drop_duplicates()\n",
    "source_df.columns = ['name', 'id']\n",
    "target_df = df[['target_name', 'target_q']].drop_duplicates()\n",
    "target_df.columns = ['name', 'id']\n",
    "all_nodes_df = pd.concat([source_df, target_df]).drop_duplicates()\n",
    "all_nodes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619df457-995a-4bf8-8f83-74aa835327bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p31(row):\n",
    "    # P31 corresponds to \"instance of\"\n",
    "    \n",
    "    itempage = pywikibot.ItemPage(wikidata, row)\n",
    "    itemdata = itempage.get()\n",
    "    try:\n",
    "        target = itemdata['claims']['P31'][0].getTarget()\n",
    "        target.get()\n",
    "        return target.labels['en']\n",
    "    except:\n",
    "        return 'Unknown'\n",
    "    \n",
    "\n",
    "def add_nodes(rows, batch_size=10000):\n",
    "    # Adds author nodes to the Neo4j graph as a batch job.\n",
    "\n",
    "    query = '''UNWIND $rows AS row\n",
    "               MERGE (:Node {name: row.name, id: row.id, type: row.node_label})\n",
    "               RETURN count(*) as total\n",
    "    '''\n",
    "    return insert_data(query, rows, batch_size)\n",
    "\n",
    "\n",
    "def add_edges(rows, batch_size=50000):\n",
    "    \n",
    "    \n",
    "    query = \"\"\"UNWIND $rows AS row\n",
    "               MATCH (src:Node {id: row.source_q}), (tar:Node {id: row.target_q})\n",
    "               CREATE (src)-[:%s]->(tar)\n",
    "    \"\"\" % edge\n",
    "    \n",
    "    return insert_data(query, rows, batch_size)\n",
    "\n",
    "\n",
    "def insert_data(query, rows, batch_size = 10000):\n",
    "    # Function to handle the updating the Neo4j database in batch mode.\n",
    "\n",
    "    total = 0\n",
    "    batch = 0\n",
    "    start = time.time()\n",
    "    result = None\n",
    "\n",
    "    while batch * batch_size < len(rows):\n",
    "\n",
    "        res = conn.query(query, parameters={'rows': rows[batch*batch_size:(batch+1)*batch_size].to_dict('records')})\n",
    "        try:\n",
    "            total += res[0]['total']\n",
    "        except:\n",
    "            total += 0\n",
    "        batch += 1\n",
    "        result = {\"total\":total, \"batches\":batch, \"time\":time.time()-start}\n",
    "        print(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a2cd0-2b76-431a-bc8a-7b46872cba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_nodes_df['node_label'] = all_nodes_df['id'].map(get_p31)\n",
    "all_nodes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c330a1-bc50-475a-bf15-a4d6eaa6d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_nodes(all_nodes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b089d-d9fb-4a93-a788-059a30d48b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_ls = df['rel_name'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d1998-0826-453c-89d4-06197c21e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge in edge_ls:\n",
    "    print(edge)\n",
    "    y = df[df['rel_name'] == edge]\n",
    "    add_edges(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b91ae-11a0-4298-8955-c824f1fa49cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"MATCH (n:Node) \n",
    "           WITH n.name AS name, COLLECT(n) AS nodes \n",
    "           WHERE SIZE(nodes)>1 \n",
    "           FOREACH (el in nodes | DETACH DELETE el)\n",
    "\"\"\"\n",
    "\n",
    "conn.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85ac65-fe7d-4664-849e-61a11580d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"MATCH (n:Node) \n",
    "           SET n.type_ls = apoc.convert.toStringList(n.type)\n",
    "\"\"\"\n",
    "\n",
    "conn.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276176da-b88b-4d87-acae-93bb6d82cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"MATCH (n:Node) \n",
    "           CALL apoc.create.addLabels(n, n.type_ls) \n",
    "           YIELD node RETURN node\n",
    "\"\"\"\n",
    "\n",
    "conn.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a66e6-e6b5-4fea-ae4b-d9d5f8b95e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
