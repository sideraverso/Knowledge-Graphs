{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "631e9b9e-4ec1-40bc-b95e-d69f383f8301",
   "metadata": {},
   "source": [
    "# Exercise 5b: Machine learning on graphs\n",
    "\n",
    "## Introduction\n",
    "\n",
    "There are a variety of machine learning tasks that we can perform on a graph.  One common use case would be code classification (is this node a fraudster or not?).  We can also do link prediction (should I recommend this item to a user?) or graph prediction (which molecule is this?), which are beyond the scope of this course.\n",
    "\n",
    "For this exercise, we will be using our NLP-based graph.  (We will recreate it below if you no longer have access to that database.)  We are going to try to do some node classification based on node embeddings, which will will create with [GDS](https://neo4j.com/docs/graph-data-science/current/algorithms/node-embeddings/).  Those embeddings will then be used in a simple ML pipeline created with `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a21e15-4a99-4e38-837a-370d193cdeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import re\n",
    "import urllib\n",
    "from pprint import pprint\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from py2neo import Node, Graph, Relationship, NodeMatcher\n",
    "from py2neo.bulk import merge_nodes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da9129-07eb-4730-841b-6b810797df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = ''\n",
    "user = 'neo4j'\n",
    "pwd = ''\n",
    "\n",
    "graph = Graph(uri, auth=(user, pwd))\n",
    "nodes_matcher = NodeMatcher(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a3dd76-6936-4b50-a7e0-e62f5bc8efe3",
   "metadata": {},
   "source": [
    "## Run this cell if you need to repopulate the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e93ec0-41ae-4e69-923e-fdf58b7c6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"\"\"CALL apoc.import.json('https://raw.githubusercontent.com/cj2001/nodes2021_kg_workshop/main/json_files/svo.json')\"\"\"\n",
    "\n",
    "query2 = \"\"\"MATCH (n:Node) \n",
    "            CALL apoc.create.addLabels(n, n.node_labels) \n",
    "            YIELD node \n",
    "            RETURN COUNT(*)\n",
    "\"\"\"\n",
    "\n",
    "query3 = \"\"\"MATCH (n:Node) \n",
    "            WITH n.name AS name, COLLECT(n) AS nodes \n",
    "            WHERE SIZE(nodes)>1 \n",
    "            FOREACH (el in nodes | DETACH DELETE el)\n",
    "\"\"\"\n",
    "\n",
    "graph.run(query1)\n",
    "graph.run(query2)\n",
    "graph.run(query3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e32778-61da-4012-bcd2-bf6fd02f2a56",
   "metadata": {},
   "source": [
    "## Just for fun (and slightly off topic): a tiny bit of entity resolution / disambiguation\n",
    "\n",
    "Let's just see what happens when we look at the overlap between two possible duplicate nodes for Barack Obama.  We could hypothesize that there would be overlap in the nodes each are connected to.  Let's see...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed2c4e-bd39-4816-81fa-46b65f32d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbo_ls = []\n",
    "pbo = graph.run('MATCH (n:Node {name: \"president barack obama\"})--(m) RETURN DISTINCT m.name')\n",
    "for record in pbo:\n",
    "    pbo_ls.append(record[0])\n",
    "print('Total number of connected nodes: ', len(pbo_ls))\n",
    "pbo_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5ff393-b29a-4d14-8642-805c2ec3d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obm_ls = []\n",
    "obm = graph.run('MATCH (n:Node {name: \"oh bah mə\"})--(m) RETURN DISTINCT m.name')\n",
    "for record in obm:\n",
    "    obm_ls.append(record[0])\n",
    "print('Total number of connected nodes: ', len(obm_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec8bb0-af84-4868-9f54-0cde6b4a658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbo_set = set(pbo_ls)\n",
    "obm_set = set(obm_ls)\n",
    "if (pbo_set & obm_set):\n",
    "    print('Number of overlapping elements: ', len(pbo_set & obm_set))\n",
    "    print('Percent of overlapping elements: ', len(pbo_set & obm_set)/len(pbo_ls))\n",
    "    print(pbo_set & obm_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cdc64c-220f-4a15-b3bf-5629fa0f44ee",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "If we choose the starting node to be `president barack obama`, then we see that there is 100% overlap with `oh bah mə`.\n",
    "\n",
    "## _EXERCISE:_ What if we reverse the order of those two?\n",
    "\n",
    "So what happens if we are comparing `oh bah mə` to `president barack obama`?  (You can probably figure this out without having to run code.)  We will look at another example below once we create node embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3850e1f-0259-42c7-9997-03b70741aa29",
   "metadata": {},
   "source": [
    "## Creating some labels\n",
    "\n",
    "Classification, being a supervised problem, requires node labels.  We have some of those already!  \n",
    "\n",
    "Let's say we wanted to determine if a given node was a person, place, or thing (abbreviated below as `pptu`, where the `u` stands for unknown).  We can use our node labels to create a toy problem for us.  \n",
    "\n",
    "I have manually created a list of node labels below and then set a propert on each node that will correspond to the label for the classification model.  These are completely arbitrary and just for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb63c98-b504-4980-a878-3a6e31917a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_person = \"\"\"MATCH (n) WHERE ANY (x in n.node_labels WHERE x IN \n",
    "                   ['Person', 'Organization', 'EducationalOrganization', 'Corporation', \n",
    "                    'SportsTeam', 'SportsOrganization', 'GovernmentOrganization']) \n",
    "               SET n.pptu_person = 1;\n",
    "\"\"\"\n",
    "           \n",
    "query_place = \"\"\"MATCH (n)\n",
    "                   WHERE ANY (x in n.node_labels WHERE x IN \n",
    "                       ['Place', 'AdministrativeArea', 'Country', 'Museum', \n",
    "                        'TouristAttraction', 'CivicStructure', 'City', 'CollegeOrUniversity',\n",
    "                        'MovieTheater', 'Continent', 'MusicVenue', 'LandmarksOrHistoricalBuildings', \n",
    "                        'Cemetery', 'BodyOfWater', 'PlaceOfWorship', 'Restaurant', \n",
    "                        'LakeBodyOfWater'])\n",
    "                SET n.pptu_place = 1;\n",
    "\"\"\"\n",
    "           \n",
    "query_thing = \"\"\"MATCH (n)\n",
    "                   WHERE ANY (x in n.node_labels WHERE x IN \n",
    "                       ['Thing', 'Periodical', 'Book', 'Movie', \n",
    "                        'Event', 'MusicComposition', 'SoftwareApplication', 'ProductMode', \n",
    "                        'DefenceEstablishment', 'MusicRecording', 'LocalBusiness', 'CreativeWork', \n",
    "                        'Article', 'TVEpisode', 'ItemList', 'TVSeries', \n",
    "                        'Airline', 'Product', 'VisualArtwork', 'VideoGame', \n",
    "                        'Brand'])\n",
    "                SET n.pptu_thing = 1;\n",
    "\"\"\"\n",
    "           \n",
    "query_unk = \"\"\"MATCH (n)\n",
    "               WHERE n.pptu_person IS NULL\n",
    "                   AND n.pptu_place IS NULL\n",
    "                   AND n.pptu_thing IS NULL\n",
    "               SET n.pptu_unknown = 1;\n",
    "\"\"\"\n",
    "\n",
    "graph.run(query_person)\n",
    "graph.run(query_place)\n",
    "graph.run(query_thing)\n",
    "graph.run(query_unk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eeedae-f988-4c7c-b58c-4e5ec1b65431",
   "metadata": {},
   "source": [
    "## Creating the in-memory graph\n",
    "\n",
    "As before, we need to create an in-memory graph through GDS, which will be used to calculate our node embeddings in subsequent steps.  In this case, as seen below, we are actually looking at _all_ nodes and _all_ relationships in the graph.  This is, in general, bad practice, especially as the graph size grows.  \n",
    "\n",
    "You should note that most of the GDS algorithms require monopartite, undirected graphs.  Our graph is not actually undirected, but we will treat it that way for the purposes of this demonstration.  When we do that, GDS essentially mimics two relationships between nodes: incoming and outgoing.  So you should see double the number of your total amount of relationships when you create this in-memory graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe868f11-1cfc-454b-bef7-8ca354af200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"CALL gds.graph.create(\n",
    "               'all_nodes',\n",
    "               {\n",
    "                    AllNodes: {label: 'Node', \n",
    "                               properties: \n",
    "                                   {word_vec_embedding: {property: 'word_vec'}}}\n",
    "               },\n",
    "               {\n",
    "                    AllRels: {type: '*', orientation: 'UNDIRECTED'}\n",
    "               }\n",
    "           )\n",
    "           YIELD graphName, nodeCount, relationshipCount\n",
    "\"\"\"\n",
    "\n",
    "graph.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de59c3-fa7f-4401-abd4-1df604fa8659",
   "metadata": {},
   "source": [
    "## node2vec\n",
    "\n",
    "We will now create some embeddings with `node2vec`.  If you recall from earlier in this workshop, `node2vec` is similar to `word2vec`, in that it uses a skip-gram approach.  What this means for us is that there will be a random walk of a certain length for each node around the graph and that will be used to create the embeddings.  \n",
    "\n",
    "#### _THOUGHT EXERCISE:_ What do you suppose will happen to nodes that are either not connected or only have a relationship or two and not connected to the bulk of the other nodes???\n",
    "\n",
    "So let's create a 10-dimensional node embedding for every node and see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81bb753-c87f-4cee-9aa8-32866d1345bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"CALL gds.beta.node2vec.stream('all_nodes', {embeddingDimension: 10}) \n",
    "           YIELD nodeId, embedding \n",
    "           RETURN gds.util.asNode(nodeId).name as name, embedding\n",
    "           LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "graph.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784dbb8f-accb-4cc2-9fa7-acb7d0ff03ed",
   "metadata": {},
   "source": [
    "## Writing these vectors to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ba73c-abc6-4d00-a7f4-e8645967f087",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"CALL gds.beta.node2vec.write('all_nodes', \n",
    "               { \n",
    "                   embeddingDimension: 10, \n",
    "                   writeProperty: 'n2v_all_nodes'\n",
    "               } \n",
    "           )\n",
    "\"\"\"\n",
    "\n",
    "graph.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698bf1af-b0f8-47b4-aa4d-ebf9d7bd8559",
   "metadata": {},
   "source": [
    "## Another entity resolution approach\n",
    "\n",
    "We now have vectors.  We can measure the distance between two vectors and use that to infer similarity.  There are many ways to do that, but let's just try the classic [cosine similarity](https://neo4j.com/docs/graph-data-science/current/alpha-algorithms/cosine/).\n",
    "\n",
    "#### For fun\n",
    "\n",
    "Try some other nodes compared to `oh bah mə`.  `mitch mcconnell` is a good one.  Can you explain that result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffeac7e-f8a0-45bb-bc48-9a82ecb13d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"MATCH (n1:Node {name: 'oh bah mə'}) \n",
    "           MATCH (n2:Node {name: 'hillary clinton'}) \n",
    "           RETURN gds.alpha.similarity.cosine(n1.n2v_all_nodes, n2.n2v_all_nodes) AS similarity\n",
    "\"\"\"\n",
    "\n",
    "graph.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d3228e-f8f6-4aa8-bba6-d315199593b2",
   "metadata": {},
   "source": [
    "## Getting started with ode classification\n",
    "\n",
    "Let's now actually do the node classification.  Pulling down our data, you will notice that each node could have multiple classification: a multiclass problem.  For the sake of demonstration (and for the significant lack of data in this small graph), we are just going to do some binary classification, to try and predict person versus not-person, place versus not-place, etc.  \n",
    "\n",
    "Also, if you were to query the data, you would see that we have a significant class imbalance.  We will be using a support vector machine for this model and the one built into `scikit-learn` (`svm.SVC`) has some ability to handle that.  In reality, for a proper dataset we would do some sampling work to try and even out the class imbalance a bit.  However, for this very small dataset, we are going to just use what we have.  In a real problem, you would be spending some significant time on that problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2969f5-17c3-49ce-88a4-2ac1101ad05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"MATCH (n:Node) \n",
    "           RETURN n.name, n.node_labels, n.pptu_person, n.pptu_place, \n",
    "           n.pptu_thing, n.pptu_unknown, n.word_vec, n.n2v_all_nodes\n",
    "\"\"\"\n",
    "\n",
    "df = graph.run(query).to_data_frame()\n",
    "df.columns = ['name', 'node_labels', 'pptu_person', 'pptu_place', \n",
    "              'pptu_thing', 'pptu_unknown', 'word_vec', 'n2v_all_nodes']\n",
    "df2 = df.fillna(0)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debc7bb0-c782-40bd-8c53-deb547bcf766",
   "metadata": {},
   "source": [
    "## Creating the code around the model\n",
    "\n",
    "This next bit is going to get our data into the format needed for the model.  Note that we will be comparing the graph embeddings to the word vectors we generated a while back.  However, not all nodes have word vectors (such as if they did not have a did not have a Wikipedia description).  In this case, we just set those vectors to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc9008-be9f-467e-ad08-5eb07f9d4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X(df2):\n",
    "\n",
    "    word_vec_ls = df2['word_vec'].to_list()\n",
    "    word_vec_arr = np.array([np.array(x) if x != 0 else np.zeros(300).tolist() for x in word_vec_ls], dtype=object)\n",
    "\n",
    "    n2v_an_ls = df2['n2v_all_nodes'].to_list()\n",
    "    n2v_arr = np.array([np.array(x) for x in n2v_an_ls], dtype=object)\n",
    "\n",
    "    print(word_vec_arr.shape, n2v_arr.shape)\n",
    "    \n",
    "    return word_vec_arr, n2v_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bad32c-544d-4f00-9d6d-89ea8c903211",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word_vec, X_all_nodes = create_X(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dea87e-55d2-437a-9427-4119a3046510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeler(df, column_name, X, k_folds=5, model='linear', show_matrix=True):\n",
    "    \n",
    "    y = df[column_name].fillna(0.0).to_numpy()\n",
    "    acc_scores = []\n",
    "    \n",
    "    pos = np.count_nonzero(y == 1.0)\n",
    "    neg = y.shape[0] - pos\n",
    "    print('Number of positive: ', pos, ' Number of negative: ', neg)\n",
    "    \n",
    "    for i in range(0, k_folds):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_word_vec, y, test_size=0.25)\n",
    "        clf = svm.SVC(kernel='linear', class_weight='balanced')\n",
    "        clf.fit(X_train, y_train)\n",
    "        pred = clf.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(pred, y_test)\n",
    "        acc_scores.append(acc)        \n",
    "        \n",
    "    print('Accuracy scores: ', acc_scores)\n",
    "    print('Mean accuracy: ', np.mean(acc_scores))\n",
    "    \n",
    "    if show_matrix:\n",
    "        matrix = plot_confusion_matrix(clf, X_test, y_test, cmap=plt.cm.Blues, normalize='true')\n",
    "        plt.show(matrix)\n",
    "        plt.show()\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a469f9b-25df-47f4-931a-3a116fbbabc8",
   "metadata": {},
   "source": [
    "## Training and running the model\n",
    "\n",
    "Let's now compare the accuracy of both the word vectors as well as the node embeddings and see how we do for each of the 4 cases..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c03903-5c05-4cf1-8d36-7c8870a1d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeler(df2, 'pptu_person', X_word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4357029-810b-4ab8-b1b6-7bcbbfce54e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeler(df2, 'pptu_person', X_all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf08ae9-1666-4944-9f5c-801151ec728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeler(df2, 'pptu_place', X_word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba6b882-45b4-4884-9cb3-1dc11b7de06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeler(df2, 'pptu_place', X_all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b544df71-6226-4b80-96f3-3e117d276e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeler(df2, 'pptu_thing', X_word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f30e40-543e-4b94-8b16-690435f610fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeler(df, 'pptu_thing', X_all_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b62964-d042-47e3-bc7c-10e25feac3cb",
   "metadata": {},
   "source": [
    "## What just happened???\n",
    "\n",
    "If you recall, `spacy` generated us 300-dimensional word embeddings for each node.  That size vector is appropriate given the size of the vocabulary for our Google Knowledge Graph-provided descriptions.  But we have also run the accuracy of the graph embeddings, _which are only 10-dimensional vectors_.  That is roughly appropriate because there are so few nodes in the graph and we should not have a higher dimension than the number of nodes.  \n",
    "\n",
    "**But notice that even with only 10 dimensions we have very comparable accuracy!!!**\n",
    "\n",
    "That is pretty cool. :)\n",
    "\n",
    "## Next steps\n",
    "\n",
    "There are so many things that you can try from here! Some of the things I might consider would be:\n",
    "\n",
    "- Take time to tune the hyperparameters. This can be done for:\n",
    "    - The spacy word embeddings\n",
    "    - The graph embeddings\n",
    "    - The ML model\n",
    "- Trying more sophisticated embedding approaches, such as GraphSAGE that takes into account the node properties.\n",
    "- Explore different embeddings. Here we used the spacy word vectors to create embeddings for the nodes, but there are many, many more ways to create vectors that could be used for training the ML models! Get creative!\n",
    "- Work the class imbalance problem.  This graph is quite small in reality. Work on growing the graph by adding more layers to it via either Wikipedia or the Google Knowledge Graph. As the graph gets larger, we might expect that the graph embeddings approaches will start to really shine beyond the word embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2706fe2-ce4a-44a4-a9b6-68b11ec72ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
