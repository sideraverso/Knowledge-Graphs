{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "022ff146-c810-4e79-967c-4a82d82dc303",
   "metadata": {},
   "source": [
    "# Exercise 2: Build your own knowledge graph with NLP\n",
    "\n",
    "## Don't worry!  An NLP workflow will be included here!\n",
    "\n",
    "There is a lot going on in this notebook.  We will walk through each step.\n",
    "\n",
    "There are a lot of functions that get the search data into a format that can be uploaded into the graph.  As with any NLP project, there is a lot of preprocessing that needs to happen before you even worry about creating the graph.  And as we discussed, there is no proverbial \"silver bullet\" when it comes to NLP.  \n",
    "\n",
    "For this exercise we will be creating the (subject, verb, object) triples.  From a broad brush-strokes perspective, this is what our NLP workflow will look like:\n",
    "\n",
    "<img src=\"images/nlp_workflow.png\" width=\"600\">\n",
    "\n",
    "## `Spacy`\n",
    "\n",
    "To achieve the above, we will us the NLP package `spacy`.  It has a lot of great, basic functionality, especially when we are talking about detecting the parts of speech and identifying the ROOT.  But of course, you can choose to use anything you are comfortable with!\n",
    "\n",
    "\n",
    "### A note about language models and configuring `spacy`\n",
    "\n",
    "We will use, just a little bit, the word vectors generated by `spacy`.  There are plenty of corpuses available from `spacy`.  We are going to use the medium-sized web corpus called `en_core_web_md`, which you can read about [here](https://spacy.io/models/en#en_core_web_md). The basic model is their small core library, taken from the web: `en_core_web_sm`, which provides good, basic functionality with a small download size (< 20 MB). However, one drawback of this basic model is that it doesn't have full word vectors. Instead, it comes with context-sensitive tensors. You can still do things like text similarity with it, but if you want to use spacy to create good word vectors, you should use a larger model such as `en_core_web_md` or `en_core_web_lg` since the small models are not known for accuracy. You can also use a variety of third-party models, but that is beyond the scope of this workshop. Again, choose the model that works best with your setup.  These will have to be loaded in before we can do any NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c608c669-e059-496d-83b4-e4951446ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import urllib\n",
    "from pprint import pprint\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from py2neo import Node, Graph, Relationship, NodeMatcher\n",
    "from py2neo.bulk import merge_nodes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c1b28c-96a7-417f-abc6-0dbb2adec1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en-core-web-md==3.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.1.0/en_core_web_md-3.1.0-py3-none-any.whl#egg=en_core_web_md==3.1.0 in /home/cj2001/notebook_env/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: spacy<3.2.0,>=3.1.0 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from en-core-web-md==3.1.0) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.26.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (1.21.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (0.3.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (8.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: setuptools in /home/cj2001/notebook_env/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (44.0.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (4.62.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: jinja2 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (21.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/cj2001/notebook_env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /home/cj2001/notebook_env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.10.0.2)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/cj2001/notebook_env/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.4.7)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1b970-7d4a-4335-9f48-6bcae4748597",
   "metadata": {},
   "source": [
    "## The Google knowledge graph API\n",
    "\n",
    "We will be building out the SVO's in our graph by querying Google for node properties.  There are obviously several ways we can build out the node properties, but I decided to show using \"free\" queries (allows a free quote up to 100,000 read calls per day per project) to the Google Knowledge Graph API just to give you a view into one of the many ways you could create your own knowledge graph.\n",
    "\n",
    "### Getting an API key\n",
    "\n",
    "We will need to create an API key to query the Google Knowledge Graph.  Visit [this link](https://developers.google.com/knowledge-graph/how-tos/authorizing) to get instructions how to do it.  This will give you \n",
    "\n",
    "<img src=\"images/google_kg_api1.png\" width=\"600\">\n",
    "\n",
    "Click on \"Credentials page\" and then you will see\n",
    "\n",
    "<img src=\"images/google_kg_api2.png\" width=\"600\">\n",
    "\n",
    "From here you will click on \"+ CREATE CREDENTIALS\".  From there, copy the key into the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58bc3513-95a7-4dd3-accd-153534a7c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyAy83OljsqiXfUaFBsJjb9jJyyfoh0891E'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5bd4d4-a2a8-4382-b17f-01fe4200bec1",
   "metadata": {},
   "source": [
    "## Let's get going with the NLP!\n",
    "\n",
    "Here we will do a few things.  First, we are going to provide a list of word dependencies that we will attribute to subjects, verbs, and objects.  We will have `spacy` look for these for populating our SVO triples.\n",
    "\n",
    "Next, we initialize `spacy` using our corpus.  We will also add to the normal pipeline a feature, `merge_noun_chunks`, which is handy for keeping names of things together (example: treat the noun as \"barack obama\" rather than \"barack\" and \"obama\" separately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3158c85b-62f5-4270-b421-e580ad235ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function spacy.pipeline.functions.merge_noun_chunks(doc: spacy.tokens.doc.Doc) -> spacy.tokens.doc.Doc>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "VERBS = ['ROOT', 'advcl']\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", 'pobj']\n",
    "ENTITY_LABELS = ['PERSON', 'NORP', 'GPE', 'ORG', 'FAC', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART']\n",
    "\n",
    "non_nc = spacy.load('en_core_web_md')\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe('merge_noun_chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5199fead-6e06-487a-9662-39822ebc098b",
   "metadata": {},
   "source": [
    "### Helper function for querying the Google Knowledge Graph\n",
    "\n",
    "Note that GKG can return several results for a single query (example: what would happen if you queried \"washington\"...are you looking for a city, state, or person?).  We will limit this to 10 returned results, but in reality we are going to use only the first one below.  But you could add all of them to the graph with some minor re-writes of the below NLP pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3ff1ad2-a94e-4a53-82d3-97d70f5f53e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_google(query, api_key, limit=10, indent=True, return_lists=True):\n",
    "    \n",
    "    text_ls = []\n",
    "    node_label_ls = []\n",
    "    url_ls = []\n",
    "    \n",
    "    params = {\n",
    "        'query': query,\n",
    "        'limit': limit,\n",
    "        'indent': indent,\n",
    "        'key': api_key,\n",
    "    }   \n",
    "    \n",
    "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "    url = service_url + '?' + urllib.parse.urlencode(params)\n",
    "    response = json.loads(urllib.request.urlopen(url).read())\n",
    "    \n",
    "    if return_lists:\n",
    "        for element in response['itemListElement']:\n",
    "\n",
    "            try:\n",
    "                node_label_ls.append(element['result']['@type'])\n",
    "            except:\n",
    "                node_label_ls.append('')\n",
    "\n",
    "            try:\n",
    "                text_ls.append(element['result']['detailedDescription']['articleBody'])\n",
    "            except:\n",
    "                text_ls.append('')\n",
    "                \n",
    "            try:\n",
    "                url_ls.append(element['result']['detailedDescription']['url'])\n",
    "            except:\n",
    "                url_ls.append('')\n",
    "                \n",
    "        return text_ls, node_label_ls, url_ls\n",
    "    \n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a98bd88-e279-45c2-82d1-e6da7f23fa68",
   "metadata": {},
   "source": [
    "## Here is the main chunk of the NLP\n",
    "\n",
    "The important function here is the last one that drives the whole thing: `create_svo_triples`.  Here is what it does:\n",
    "\n",
    "1. Remove special characters (`remove_special_characters`)\n",
    "2. Remove stop words and punctuation (`remove_stop_words_and_punct`)\n",
    "3. Remove dates (`remove_dates`)\n",
    "4. Remove duplicates (`remove_duplicates`)\n",
    "\n",
    "Duplicates crop up many times during this process and we will be battling them a lot!\n",
    "\n",
    "Then, we can get to the heart of the matter, `create_svo_triples`, which returns a list of tuples of all SVO triples in the text, like\n",
    "\n",
    "```\n",
    "[('oh bah mÉ™', 'be', 'american politician'),\n",
    " ('oh bah mÉ™', 'be', '44th president'),\n",
    " ('oh bah mÉ™', 'be', 'united states')]\n",
    " ```\n",
    " \n",
    "One key thing about this function is how we are adding the verbs.  There is no right or wrong way to do this.  What is implemented here is to try and figure out which verb is closest in distance between each object and each verb.  Definitely experiment with this and see if you can come up with a better way to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40a276fb-bcf4-48ea-94c8-0bdd032e5679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    \n",
    "    regex = re.compile(r'[\\n\\r\\t]')\n",
    "    clean_text = regex.sub(\" \", text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def remove_stop_words_and_punct(text, print_text=False):\n",
    "    \n",
    "    result_ls = []\n",
    "    rsw_doc = non_nc(text)\n",
    "    \n",
    "    for token in rsw_doc:\n",
    "        if print_text:\n",
    "            print(token, token.is_stop)\n",
    "            print('--------------')\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            result_ls.append(str(token))\n",
    "    \n",
    "    result_str = ' '.join(result_ls)\n",
    "\n",
    "    return result_str\n",
    "\n",
    "\n",
    "def create_svo_lists(doc, print_lists):\n",
    "    \n",
    "    subject_ls = []\n",
    "    verb_ls = []\n",
    "    object_ls = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ in SUBJECTS:\n",
    "            subject_ls.append((token.lower_, token.idx))\n",
    "        elif token.dep_ in VERBS:\n",
    "            verb_ls.append((token.lemma_, token.idx))\n",
    "        elif token.dep_ in OBJECTS:\n",
    "            object_ls.append((token.lower_, token.idx))\n",
    "\n",
    "    if print_lists:\n",
    "        print('SUBJECTS: ', subject_ls)\n",
    "        print('VERBS: ', verb_ls)\n",
    "        print('OBJECTS: ', object_ls)\n",
    "    \n",
    "    return subject_ls, verb_ls, object_ls\n",
    "\n",
    "\n",
    "def remove_duplicates(tup, tup_posn):\n",
    "    \n",
    "    check_val = set()\n",
    "    result = []\n",
    "    \n",
    "    for i in tup:\n",
    "        if i[tup_posn] not in check_val:\n",
    "            result.append(i)\n",
    "            check_val.add(i[tup_posn])\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "def remove_dates(tup_ls):\n",
    "    \n",
    "    clean_tup_ls = []\n",
    "    for entry in tup_ls:\n",
    "        if not entry[2].isdigit():\n",
    "            clean_tup_ls.append(entry)\n",
    "    return clean_tup_ls\n",
    "\n",
    "\n",
    "def create_svo_triples(text, print_lists=False):\n",
    "    \n",
    "    clean_text = remove_special_characters(text)\n",
    "    doc = nlp(clean_text)\n",
    "    subject_ls, verb_ls, object_ls = create_svo_lists(doc, print_lists=print_lists)\n",
    "    \n",
    "    graph_tup_ls = []\n",
    "    dedup_tup_ls = []\n",
    "    clean_tup_ls = []\n",
    "    \n",
    "    for subj in subject_ls: \n",
    "        for obj in object_ls:\n",
    "            \n",
    "            dist_ls = []\n",
    "            \n",
    "            for v in verb_ls:\n",
    "                \n",
    "                # Assemble a list of distances between each object and each verb\n",
    "                dist_ls.append(abs(obj[1] - v[1]))\n",
    "                \n",
    "            # Get the index of the verb with the smallest distance to the object \n",
    "            # and return that verb\n",
    "            index_min = min(range(len(dist_ls)), key=dist_ls.__getitem__)\n",
    "            \n",
    "            # Remve stop words from subjects and object.  Note that we do this a bit\n",
    "            # later down in the process to allow for proper sentence recognition.\n",
    "\n",
    "            no_sw_subj = remove_stop_words_and_punct(subj[0])\n",
    "            no_sw_obj = remove_stop_words_and_punct(obj[0])\n",
    "            \n",
    "            # Add entries to the graph iff neither subject nor object is blank\n",
    "            if no_sw_subj and no_sw_obj:\n",
    "                tup = (no_sw_subj, verb_ls[index_min][0], no_sw_obj)\n",
    "                graph_tup_ls.append(tup)\n",
    "        \n",
    "        #clean_tup_ls = remove_dates(graph_tup_ls)\n",
    "    \n",
    "    dedup_tup_ls = remove_duplicates(graph_tup_ls, 2)\n",
    "    clean_tup_ls = remove_dates(dedup_tup_ls)\n",
    "    \n",
    "    return clean_tup_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa6dfcd-ec7d-47b2-ab12-c969335b01f5",
   "metadata": {},
   "source": [
    "## Now we are going to make this data more interesting!\n",
    "\n",
    "We have a whole ton of objects within our data.  Each sentence might have several linked to the subject.  Let's get some properties of each object, through our `query_google_`, to get their Google description, node labels, and any URL they might have.  Not all objects will have this information, but we can always add the ones that do as node properties in our graph.\n",
    "\n",
    "This code block also does a few other things.  For example, `add_layer` will go out and create SVOs using the above `create_svo_triples` for each object within our starting list, which adds more nodes and edges to our graph.  We do a bit more cleaning by removing tuples where the subject and object are equal (`subj_equals_obj`...it does happen).  Lastly, we are going to take any descriptions associated with the objects and turn them into the `spacy` word vectors (`create_word_vectors`), under the idea that they might be useful down the road.  (Those without descriptions get an array of 0's for their vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69a12332-8187-4b2e-9164-06c6729a54ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_obj_properties(tup_ls):\n",
    "    \n",
    "    init_obj_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "\n",
    "        try:\n",
    "            text, node_label_ls, url = query_google(tup[2], api_key, limit=1)\n",
    "            new_tup = (tup[0], tup[1], tup[2], text[0], node_label_ls[0], url[0])\n",
    "        except:\n",
    "            new_tup = (tup[0], tup[1], tup[2], [], [], [])\n",
    "        \n",
    "        init_obj_tup_ls.append(new_tup)\n",
    "        \n",
    "    return init_obj_tup_ls\n",
    "\n",
    "\n",
    "def add_layer(tup_ls):\n",
    "\n",
    "    svo_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        \n",
    "        if tup[3]:\n",
    "            svo_tup = create_svo_triples(tup[3])\n",
    "            svo_tup_ls.extend(svo_tup)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return get_obj_properties(svo_tup_ls)\n",
    "        \n",
    "\n",
    "def subj_equals_obj(tup_ls):\n",
    "    \n",
    "    new_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        if tup[0] != tup[2]:\n",
    "            new_tup_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4], tup[5]))\n",
    "            \n",
    "    return new_tup_ls\n",
    "\n",
    "\n",
    "def check_for_string_labels(tup_ls):\n",
    "    # This is for an edge case where the object does not get fully populated\n",
    "    # resulting in the node labels being assigned to string instead of list.\n",
    "    # This may not be strictly necessary and the lines using it are commnted out\n",
    "    # below.  Run this function if you come across this case.\n",
    "    \n",
    "    clean_tup_ls = []\n",
    "    \n",
    "    for el in tup_ls:\n",
    "        if isinstance(el[2], list):\n",
    "            clean_tup_ls.append(el)\n",
    "            \n",
    "    return clean_tup_ls\n",
    "\n",
    "\n",
    "def create_word_vectors(tup_ls):\n",
    "\n",
    "    new_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        if tup[3]:\n",
    "            doc = nlp(tup[3])\n",
    "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], doc.vector)\n",
    "        else:\n",
    "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], np.random.uniform(low=-1.0, high=1.0, size=(300,)))\n",
    "        new_tup_ls.append(new_tup)\n",
    "        \n",
    "    return new_tup_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b796960-337f-44e4-a11d-e7ad66ce2185",
   "metadata": {},
   "source": [
    "### Just some more deduping and reformatting data into what Neo4j will accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13510186-9807-4a2c-a184-9ef04e4029c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dedup(tup_ls):\n",
    "    \n",
    "    visited = set()\n",
    "    output_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        if not tup[0] in visited:\n",
    "            visited.add(tup[0])\n",
    "            output_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4]))\n",
    "            \n",
    "    return output_ls\n",
    "\n",
    "\n",
    "def convert_vec_to_ls(tup_ls):\n",
    "    \n",
    "    vec_to_ls_tup = []\n",
    "    \n",
    "    for el in tup_ls:\n",
    "        vec_ls = [float(v) for v in el[4]]\n",
    "        tup = (el[0], el[1], el[2], el[3], vec_ls)\n",
    "        vec_to_ls_tup.append(tup)\n",
    "        \n",
    "    return vec_to_ls_tup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2abe13-4b87-4ed3-b93b-6fb339ff200c",
   "metadata": {},
   "source": [
    "## Adding the tuples to the Neo4j database\n",
    "\n",
    "These next two functions use `py2neo` to add the nodes and edges, in the form of node and edge lists, to the database.  Adding the nodes (`add_nodes`) is very straight forward and uses the [bulk data operations](https://py2neo.org/2021.0/bulk/index.html) in `py2neo` to add them efficiently.\n",
    "\n",
    "Adding edges is a bit more complicated.  If they all had the same edge label we could use the bulk loader to upload them.  However, since there are many different verbs in the SVO's, we need to add them individually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c7de7d2-13fa-44ed-8ee3-401281b4fefe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_nodes(tup_ls):   \n",
    "\n",
    "    keys = ['name', 'description', 'node_labels', 'url', 'word_vec']\n",
    "    merge_nodes(graph.auto(), tup_ls, ('Node', 'name'), keys=keys)\n",
    "    print('Number of nodes in graph: ', graph.nodes.match('Node').count())\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def add_edges(edge_ls):\n",
    "    \n",
    "    edge_dc = {} \n",
    "    \n",
    "    # Group tuple by verb\n",
    "    # Result: {verb1: [(sub1, v1, obj1), (sub2, v2, obj2), ...],\n",
    "    #          verb2: [(sub3, v3, obj3), (sub4, v4, obj4), ...]}\n",
    "    \n",
    "    for tup in edge_ls: \n",
    "        if tup[1] in edge_dc: \n",
    "            edge_dc[tup[1]].append((tup[0], tup[1], tup[2])) \n",
    "        else: \n",
    "            edge_dc[tup[1]] = [(tup[0], tup[1], tup[2])] \n",
    "    \n",
    "    for edge_labels, tup_ls in tqdm(edge_dc.items()):   # k=edge labels, v = list of tuples\n",
    "        \n",
    "        tx = graph.begin()\n",
    "        \n",
    "        for el in tup_ls:\n",
    "            source_node = nodes_matcher.match(name=el[0]).first()\n",
    "            target_node = nodes_matcher.match(name=el[2]).first()\n",
    "            if not source_node:\n",
    "                source_node = Node('Node', name=el[0])\n",
    "                tx.create(source_node)\n",
    "            if not target_node:\n",
    "                try:\n",
    "                    target_node = Node('Node', name=el[2], node_labels=el[4], url=el[5], word_vec=el[6])\n",
    "                    tx.create(target_node)\n",
    "                except:\n",
    "                    continue\n",
    "            try:\n",
    "                rel = Relationship(source_node, edge_labels, target_node)\n",
    "            except:\n",
    "                continue\n",
    "            tx.create(rel)\n",
    "        tx.commit()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e7437-024f-40bc-af2c-89f96cf590d5",
   "metadata": {},
   "source": [
    "## Creating our node and edge lists: where the NLP pipeline is put to work\n",
    "\n",
    "We will be creating our node and edge lists as tuples.  What is important here (since all of the functions are described above) is the output of each function.\n",
    "\n",
    "`edge_tuple_creation` takes the raw text from the Wikipedia query below and creates the edge list, which is a list of tuples of the format:\n",
    "\n",
    "```\n",
    "(subject, verb, object, object description, node label list, url, word vector of description)\n",
    "```\n",
    "\n",
    "`node_tuple_creation`, on the other hand, has some of the same properties as the full edge list above.  But it is taking just the objects (with the search subject appended) and creates the node list of the format\n",
    "\n",
    "```\n",
    "(object, object description, node label list, url, word vector of description)\n",
    "```\n",
    "\n",
    "In this example, we are going to do one round of `add_layer` to make our edge list bigger, but you can do this technically as many times you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a208e78-37a8-4cd4-afc9-7ad2a5f1b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_tuple_creation(text):\n",
    "    \n",
    "    initial_tup_ls = create_svo_triples(text)\n",
    "    init_obj_tup_ls = get_obj_properties(initial_tup_ls)\n",
    "    new_layer_ls = add_layer(init_obj_tup_ls)\n",
    "    starter_edge_ls = init_obj_tup_ls + new_layer_ls\n",
    "    edge_ls = subj_equals_obj(starter_edge_ls)\n",
    "    edges_word_vec_ls = create_word_vectors(edge_ls)\n",
    "    \n",
    "    return edges_word_vec_ls\n",
    "\n",
    "\n",
    "def node_tuple_creation(edges_word_vec_ls):\n",
    "    \n",
    "    orig_node_tup_ls = [(edges_word_vec_ls[0][0], '', ['Subject'], '', np.random.uniform(low=-1.0, high=1.0, size=(300,)))]\n",
    "    obj_node_tup_ls = [(tup[2], tup[3], tup[4], tup[5], tup[6]) for tup in edges_word_vec_ls]\n",
    "    full_node_tup_ls = orig_node_tup_ls + obj_node_tup_ls\n",
    "    cleaned_node_tup_ls = check_for_string_labels(full_node_tup_ls)\n",
    "    #dedup_node_tup_ls = dedup(cleaned_node_tup_ls)\n",
    "    dedup_node_tup_ls = cleaned_node_tup_ls\n",
    "    node_tup_ls = convert_vec_to_ls(dedup_node_tup_ls)\n",
    "    \n",
    "    return node_tup_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06c6c1-9241-4dd6-9d2c-17e966918561",
   "metadata": {},
   "source": [
    "## Time to populate the graph!\n",
    "\n",
    "Be sure you start up a \"Blank Graph Data Science\" [Sandbox instance](https://sandbox.neo4j.com/) and get its URL and password (the user name is always `neo4j`).  We will use `py2neo` to make the connection and then instantiate the [`NodeMatcher`](https://py2neo.org/v5/matching.html#node-matching), which is used to locate nodes efficiently within the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "623331b4-d7a0-4989-866d-ae23be7bd485",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = ''\n",
    "pwd = ''\n",
    "\n",
    "graph = Graph(url, auth=('neo4j', pwd))\n",
    "nodes_matcher = NodeMatcher(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ebd976-ea3d-4090-b6fa-f7e9c8b87da1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting the text and assembling the tuple lists\n",
    "\n",
    "For fun we are going to use two related searches.  You could do only one if you wanted, but I will tell you in class on why I used these two different search terms.  (Spoiler alert: do you remember that bit about having SMEs involved in your graph???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f2ccb254-53cc-4e55-b72c-badde87b7771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 591 ms, total: 1min 16s\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "barack_text = wikipedia.summary('barack obama')\n",
    "barack_edges_word_vec_ls = edge_tuple_creation(barack_text)\n",
    "barack_node_tup_ls = node_tuple_creation(barack_edges_word_vec_ls)\n",
    "\n",
    "michelle_text = wikipedia.summary('michelle obama')\n",
    "michelle_edges_word_vec_ls = edge_tuple_creation(michelle_text)\n",
    "michelle_node_tup_ls = node_tuple_creation(michelle_edges_word_vec_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9523d73-32aa-4780-8558-742878ac6aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "866 665\n"
     ]
    }
   ],
   "source": [
    "full_node_ls = barack_node_tup_ls + michelle_node_tup_ls\n",
    "full_edge_ls = barack_edges_word_vec_ls + michelle_edges_word_vec_ls\n",
    "full_dedup_node_tup_ls = dedup(full_node_ls)\n",
    "print(len(full_node_ls), len(full_dedup_node_tup_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca8223d7-f5af-45a1-97b8-a3f0ea7d88a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in graph:  665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/92 [00:00<?, ?it/s]/tmp/ipykernel_270253/1246696937.py:45: DeprecationWarning: The transaction.commit() method is deprecated, use graph.commit(transaction) instead\n",
      "  tx.commit()\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92/92 [03:25<00:00,  2.23s/it]\n"
     ]
    }
   ],
   "source": [
    "add_nodes(full_dedup_node_tup_ls)\n",
    "add_edges(full_edge_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7335692-27ab-4a50-aa1e-96b189bbefa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
